# Roadmap de Evoluci√≥n y Futuras Capacidades

## 1. Estado Actual (Arquitectura MCP-Nativa)

La arquitectura actual del proyecto ha completado con √©xito la migraci√≥n a un **ecosistema nativo de MCP (Model Context Protocol)**. Este hito es el fundamento de la versi√≥n actual y representa un salto cualitativo en la madurez del sistema.

**Logros Clave Completados:**
-   **Desacoplamiento Total:** El agente (`agent-host`) est√° completamente aislado de la implementaci√≥n y las credenciales de sus herramientas (como la base de datos).
-   **Arquitectura de Sidecars:** La comunicaci√≥n se realiza a trav√©s de servicios especializados (`mcp-mysql-sidecar`), mejorando la seguridad, el aislamiento de fallos y la escalabilidad.
-   **Optimizaci√≥n de Latencia:** Se han implementado patrones de "Light Mode" para la carga de APIs y una gesti√≥n eficiente del ciclo de vida del agente.

Con esta base s√≥lida, el roadmap se enfoca en expandir la inteligencia, la fiabilidad y el rendimiento del sistema.

---

## 2. Fases Futuras

### Fase 1: Fiabilidad y Memoria a Largo Plazo üß†

El objetivo de esta fase es dotar al agente de una memoria persistente real, permitiendo conversaciones de m√∫ltiples turnos que sobrevivan a reinicios y errores.

-   **[ ] Checkpointing con Redis:** Integrar `langgraph-checkpoint-redis` para guardar el estado del grafo de conversaci√≥n despu√©s de cada paso.
    -   **Beneficio:** Si una API o consulta falla, se puede reintentar solo ese paso. Permite conversaciones verdaderamente largas y contextuales, especialmente en canales como WhatsApp.
-   **[ ] Cola de Tareas Persistente:** Migrar las `BackgroundTasks` de FastAPI a un sistema de colas m√°s robusto como Celery o ARQ para garantizar la entrega de respuestas incluso si el `agent-host` se reinicia.

### Fase 2: Inteligencia de Enrutamiento y Eficiencia üö¶

El objetivo es optimizar costos y latencia utilizando el modelo de lenguaje (LLM) adecuado para cada tarea.

-   **[ ] Router de V√≠a R√°pida (Fast-Path):** Implementar un nodo de enrutamiento inicial que identifique tareas simples (saludos, preguntas repetidas, queries sencillas) y las dirija a un LLM m√°s peque√±o y r√°pido (ej. `GPT-4o-mini`, `Llama-3-8B`).
-   **[ ] Router de V√≠a Lenta (Slow-Path):** Las consultas anal√≠ticas complejas que requieran un razonamiento profundo seguir√°n siendo manejadas por modelos m√°s potentes (`DeepSeek-V3`, `GPT-4o`), priorizando la precisi√≥n sobre la velocidad.

### Fase 3: Expansi√≥n del Ecosistema de Herramientas (Sidecars) üõ†Ô∏è

El objetivo es expandir las capacidades del agente a√±adiendo nuevos "brazos" especializados.

-   **[ ] Sidecar de Sistema de Archivos:** Crear un `mcp-filesystem-sidecar` que exponga herramientas para leer y escribir archivos en un volumen seguro. Esto har√≠a realidad la feature de "PDF Reader" de una forma robusta y aislada.
-   **[ ] Sidecar Gen√©rico de APIs REST:** Desarrollar un sidecar configurable que pueda realizar llamadas a cualquier API REST de terceros. El `agent-host` simplemente le pedir√≠a "llama al endpoint X de la API Y", y el sidecar se encargar√≠a de la autenticaci√≥n y la comunicaci√≥n.

### Fase 4: Optimizaci√≥n de Inferencia y Rendimiento ‚ö°

Esta fase se enfoca en llevar el rendimiento al siguiente nivel para casos de uso de alta demanda.

-   **[ ] Cach√© Sem√°ntico con VectorDB:** Implementar un sistema de cach√© que almacene los resultados de las preguntas no por el texto exacto, sino por su significado sem√°ntico (vectores).
    -   **Beneficio:** Las preguntas "¬øCu√°nto vendimos ayer?" y "dame las ventas del d√≠a anterior" golpear√≠an el mismo cach√©, reduciendo dr√°sticamente las consultas repetidas a la base de datos y el uso de LLMs.
-   **[ ] Inferencia Local (vLLM / Ollama):** Para m√°xima privacidad y m√≠nima latencia, el roadmap contempla la capacidad de desplegar modelos open-source (como Llama-3 o Mixtral) en infraestructura propia utilizando servidores de inferencia optimizados.