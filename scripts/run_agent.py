import asyncio
import sys
import os

# Ajuste de path para encontrar 'src'
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

# ‚úÖ NUEVO: Importamos HumanMessage para guardar lo que dice el usuario
from langchain_core.messages import HumanMessage
from sql_agent.graph import build_graph

async def main():
    print("--- ü§ñ SQL AGENT OSS (DeepSeek Powered) ---")
    print("Iniciando sistemas...")
    
    # Construimos el cerebro
    agent = build_graph()
    
    # ‚úÖ MEMORIA A LARGO PLAZO (Mientras dure el script)
    chat_history = [] 
    
    print("‚úÖ Agente listo. Escribe 'salir' para terminar.\n")
    
    while True:
        try:
            user_input = input("USER > ")
            if user_input.lower() in ["salir", "exit", "quit"]:
                print("üëã Hasta luego!")
                break
            
            print("‚è≥ Pensando...")
            
            # 1. Guardamos tu pregunta en la historia
            chat_history.append(HumanMessage(content=user_input))
            
            # 2. Le pasamos TODA la historia al agente
            inputs = {
                "question": user_input, 
                "messages": chat_history 
            }
            
            # Ejecutamos el grafo
            result = await agent.ainvoke(inputs)
            
            # 3. Actualizamos la historia con la respuesta del Agente
            # result["messages"] contiene la lista actualizada (Tu pregunta + Respuesta IA)
            chat_history = result["messages"]
            
            # Extraemos el √∫ltimo mensaje para mostrarlo
            final_response = chat_history[-1].content
            
            print(f"\nü§ñ AI > {final_response}\n")
            print("-" * 50)
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())