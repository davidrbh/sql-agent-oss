"""Punto de entrada principal para la interfaz de usuario (Chainlit).

Este mÃ³dulo gestiona la lÃ³gica de la conversaciÃ³n mediante Web UI, integrando
el nÃºcleo del agente, el descubrimiento de herramientas y la persistencia
del estado en PostgreSQL.
"""

import sys
import os
import asyncio
import logging
import chainlit as cl
from langchain_core.messages import HumanMessage, AIMessage

from core.application.container import Container
from core.application.workflows.graph import build_graph
from features.sql_analysis.loader import get_sql_system_prompt

# Asegurar la correcta resoluciÃ³n de rutas para el paquete 'src'
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

logger = logging.getLogger("ui.main")


@cl.on_chat_start
async def on_chat_start():
    """
    Inicializa la sesiÃ³n de chat, cargando herramientas y configurando el grafo.
    """
    msg = cl.Message(content="ğŸ”Œ Conectando con el ecosistema de micro-agentes (MCP)...")
    await msg.send()

    try:
        # Obtener dependencias desde el contenedor global
        tool_provider = Container.get_tool_provider()
        checkpointer_manager = Container.get_checkpointer()
        
        msg.content = "âœ… ConexiÃ³n establecida. Cargando herramientas y memoria..."
        await msg.update()

        tools = await tool_provider.get_tools()
        system_prompt = get_sql_system_prompt(channel="web")
        
        tool_names = [t.name for t in tools]
        msg.content = f"ğŸ”§ Herramientas cargadas: {tool_names}. Configurando persistencia..."
        await msg.update()

        # ConstrucciÃ³n del grafo con persistencia transaccional
        async with checkpointer_manager.get_saver() as saver:
            graph = build_graph(tools, system_prompt, checkpointer=saver)
            cl.user_session.set("graph", graph)
            
        cl.user_session.set("history", [])

        msg.content = """ğŸ‘‹ **Â¡Hola! Soy SQL Agent v4.0 (SOA Ready)**
        
Estoy operando bajo una arquitectura orientada a servicios y persistencia robusta.
Puedo ayudarte a:
* ğŸ“Š Consultar datos histÃ³ricos SQL con validaciÃ³n AST.
* ğŸ”Œ Interactuar con mÃºltiples micro-servicios MCP.
* ğŸ’¾ Mantener el contexto de nuestra charla incluso tras reinicios.

_Â¿QuÃ© consulta deseas realizar?_"""
        await msg.update()

    except Exception as e:
        logger.error(f"Error fatal en inicio de chat: {e}")
        msg.content = f"âŒ **Error Fatal:** No se pudo inicializar el entorno.\n\nError: {e}"
        await msg.update()


@cl.on_chat_end
async def on_chat_end():
    """
    Gestiona la limpieza de recursos al finalizar la sesiÃ³n.
    Nota: Los recursos globales persisten en el Container para su reutilizaciÃ³n.
    """
    pass


@cl.on_message
async def on_message(message: cl.Message):
    """
    Manejador principal de mensajes con soporte para streaming y feedback visual.
    """
    graph = cl.user_session.get("graph")
    history = cl.user_session.get("history")
    
    if graph is None or history is None:
        await cl.Message(content="âš ï¸ Error de conexiÃ³n inicial. Reinicia el chat.").send()
        return

    status_msg = cl.Message(content="ğŸ”„ _Iniciando..._")
    await status_msg.send()

    final_response_msg = cl.Message(content="")
    final_answer_started = False
    full_response_text = ""

    try:
        # AÃ±adir mensaje actual al historial
        history.append(HumanMessage(content=message.content))
        
        inputs = {"messages": history}
        config = {
            "configurable": {"thread_id": cl.context.session.id},
            "recursion_limit": 50
        }
        
        async for event in graph.astream_events(inputs, config=config, version="v2"):
            kind = event["event"]
            name = event.get("name", "")
            data = event.get("data", {})
            
            # GestiÃ³n de la barra de estado efÃ­mera
            if kind == "on_chain_start":
                if name == "intent_classifier_node":
                    status_msg.content = "ğŸš¦ _Clasificando IntenciÃ³n..._"
                    await status_msg.update()
                elif name == "agent":
                    status_msg.content = "ğŸ§  _Generando Respuesta..._"
                    await status_msg.update()
                    
            elif kind == "on_tool_start":
                status_msg.content = f"ğŸ› ï¸ _Ejecutando Herramienta: {name}..._"
                await status_msg.update()
                
            elif kind == "on_tool_end":
                status_msg.content = "âœ… _Procesando resultados..._"
                await status_msg.update()

            # Streaming de la respuesta final del modelo
            elif kind == "on_chat_model_stream":
                node_name = event.get("metadata", {}).get("langgraph_node", "")
                
                if node_name == "agent" or not node_name:
                    chunk_content = data["chunk"].content
                    if chunk_content:
                        if not final_answer_started:
                            await status_msg.remove()
                            final_answer_started = True
                            await final_response_msg.send()
                        
                        await final_response_msg.stream_token(chunk_content)
                        full_response_text += chunk_content

        if not final_answer_started:
            await status_msg.remove()
            if not full_response_text:
                await cl.Message(content="âœ… Proceso completado.").send()
        else:
            await final_response_msg.update()
        
        if full_response_text:
            history.append(AIMessage(content=full_response_text))
            cl.user_session.set("history", history)
            
    except Exception as e:
        logger.error(f"Error procesando mensaje: {e}")
        if not final_answer_started:
            status_msg.content = f"âŒ **Error:** {str(e)}"
            await status_msg.update()
        else:
            await cl.Message(content="âŒ OcurriÃ³ un error durante la respuesta.").send()